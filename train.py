# -*- coding: utf-8 -*-
# -------------------------------------------------------------
# Jamo ê¸°ë°˜ TrOCR í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
# - ViT ì¸ì½”ë” + Transformer ë””ì½”ë” êµ¬ì¡°ë¥¼ ê°€ì§„ JamoTrOCR í•™ìŠµ
# - ìëª¨ ë‹¨ìœ„ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ ë¬¸ì ì¸ì‹ ìˆ˜í–‰
# - í•™ìŠµ ë°ì´í„°ì…‹ê³¼ ê²€ì¦ ë°ì´í„°ì…‹ì„ ë¶„ë¦¬í•˜ì—¬ ë°˜ë³µ í•™ìŠµ
# - ì˜¤ë‹µ ì¬í•™ìŠµ(incorrect sample retraining) ì „ëµ í¬í•¨
# -------------------------------------------------------------

import torch
from torch.utils.data import Dataset, DataLoader, Subset
from torchvision import transforms
from PIL import Image
from model.trocr_model import JamoTrOCR
from tokenizer.jamo_tokenizer import JamoTokenizer
import torch.optim as optim
import os
import glob
import random
import gc
import re


# -------------------------------------------------------------
# OCRDataset í´ë˜ìŠ¤
# -------------------------------------------------------------
# - ì´ë¯¸ì§€ íŒŒì¼ê³¼ í…ìŠ¤íŠ¸ ë¼ë²¨ì„ í•œ ìŒìœ¼ë¡œ êµ¬ì„±
# - í† í¬ë‚˜ì´ì €ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìëª¨ ë‹¨ìœ„ í† í° ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
# - ë””ì½”ë” ì…ë ¥([BOS] ì œì™¸)ê³¼ ë””ì½”ë” íƒ€ê¹ƒ([EOS] ì œì™¸)ì„ ë¶„ë¦¬ ë°˜í™˜
# -------------------------------------------------------------
class OCRDataset(Dataset):
    def __init__(self, img_paths, labels, tokenizer, transform=None):
        self.img_paths = img_paths
        self.labels = labels
        self.tokenizer = tokenizer
        self.transform = transform

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):
        # ì´ë¯¸ì§€ ë¡œë“œ ë° RGB ë³€í™˜
        img = Image.open(self.img_paths[idx]).convert("RGB")
        if self.transform:
            img = self.transform(img)

        # í…ìŠ¤íŠ¸ ë¼ë²¨ ì¸ì½”ë”© (ìëª¨ ë‹¨ìœ„)
        label_text = self.labels[idx]
        label_ids = self.tokenizer.encode(label_text)

        # ë””ì½”ë” ì…ë ¥ / íƒ€ê¹ƒ ë¶„ë¦¬
        decoder_input_ids = label_ids[:-1]  # [BOS] ì œì™¸
        labels = label_ids[1:]              # [EOS] ì œì™¸

        return img, torch.tensor(decoder_input_ids), torch.tensor(labels)


# -------------------------------------------------------------
# collate_fn
# -------------------------------------------------------------
# - DataLoaderì—ì„œ batch ë‹¨ìœ„ë¡œ ë°ì´í„° ë³‘í•© ì‹œ í˜¸ì¶œ
# - ê° ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•´ íŒ¨ë”©(padding) ì²˜ë¦¬ ìˆ˜í–‰
# -------------------------------------------------------------
def collate_fn(batch):
    imgs, dec_inp_ids, labels = zip(*batch)

    # ì´ë¯¸ì§€ í…ì„œ ë³‘í•©
    imgs = torch.stack(imgs)

    # í† í° ì‹œí€€ìŠ¤ íŒ¨ë”©
    dec_inp_ids = torch.nn.utils.rnn.pad_sequence(dec_inp_ids, batch_first=True, padding_value=0)
    padded_labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)

    # CrossEntropyLossì—ì„œ ë¬´ì‹œí•´ì•¼ í•  [BOS] í† í° ì²˜ë¦¬
    BOS_TOKEN_ID = tokenizer.bos_token_id
    padded_labels[padded_labels == BOS_TOKEN_ID] = -100

    return imgs, dec_inp_ids, padded_labels


# -------------------------------------------------------------
# load_data í•¨ìˆ˜
# -------------------------------------------------------------
# - ë°ì´í„° í´ë”ì—ì„œ ì´ë¯¸ì§€ì™€ ë¼ë²¨ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜´
# - íŒŒì¼ëª…ê³¼ ë¼ë²¨ ë§¤ì¹­ í™•ì¸ ë° ì •ë ¬ ìˆ˜í–‰
# -------------------------------------------------------------
def load_data(data_dir, tokenizer):
    label_dict = {}
    label_path = os.path.join(data_dir, 'labels.txt')

    # ë¼ë²¨ íŒŒì¼ íŒŒì‹±
    with open(label_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            parts = line.split('\t')
            if len(parts) < 2:
                print(f"âš ï¸ ë¬¸ì œ ìˆëŠ” ë¼ì¸ {i}: '{line}'")
                continue
            filename, label = parts[0], parts[1]
            label_dict[filename] = label

    # ì´ë¯¸ì§€ ê²½ë¡œ ìˆ˜ì§‘
    img_paths = []
    for ext in ('*.jpg', '*.jpeg', '*.png'):
        img_paths.extend(glob.glob(os.path.join(data_dir, 'images', ext)))

    # íŒŒì¼ëª… ìˆ«ì ê¸°ì¤€ ì •ë ¬
    def numerical_sort_key(s):
        numbers = re.findall(r'\d+', os.path.basename(s))
        return int(numbers[0]) if numbers else -1

    img_paths = sorted(img_paths, key=numerical_sort_key)

    img_paths_sorted, labels_sorted = [], []
    for path in img_paths:
        filename = os.path.basename(path)
        if filename in label_dict:
            img_paths_sorted.append(path)
            labels_sorted.append(label_dict[filename])
        else:
            print(f"âš ï¸ '{filename}'ì— í•´ë‹¹í•˜ëŠ” ë¼ë²¨ì´ ì—†ìŠµë‹ˆë‹¤.")

    assert len(img_paths_sorted) == len(labels_sorted), f"ì´ë¯¸ì§€ ìˆ˜({len(img_paths_sorted)})ì™€ ë¼ë²¨ ìˆ˜({len(labels_sorted)})ê°€ ë‹¤ë¦…ë‹ˆë‹¤."
    return img_paths_sorted, labels_sorted


# -------------------------------------------------------------
# train_step í•¨ìˆ˜
# -------------------------------------------------------------
# - ëª¨ë¸ í•™ìŠµìš© ë‹¨ì¼ ë°°ì¹˜ ì²˜ë¦¬
# - forward â†’ loss ê³„ì‚° â†’ backward â†’ optimizer ì—…ë°ì´íŠ¸
# -------------------------------------------------------------
def train_step(model, optimizer, loss_fn, imgs, dec_inp_ids, labels, device, pad_token_id):
    model.train()
    imgs, dec_inp_ids, labels = imgs.to(device), dec_inp_ids.to(device), labels.to(device)
    optimizer.zero_grad()

    # ëª¨ë¸ forward
    logits = model(imgs, dec_inp_ids, pad_token_id)

    # ì†ì‹¤ ê³„ì‚°
    loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))
    loss.backward()
    optimizer.step()
    return loss.item(), logits


# -------------------------------------------------------------
# eval_step í•¨ìˆ˜
# -------------------------------------------------------------
# - ê²€ì¦(Validation) ë‹¨ê³„ì—ì„œ ì†ì‹¤ ê³„ì‚°
# -------------------------------------------------------------
def eval_step(model, loss_fn, imgs, dec_inp_ids, labels, device, pad_token_id):
    model.eval()
    imgs, dec_inp_ids, labels = imgs.to(device), dec_inp_ids.to(device), labels.to(device)
    with torch.no_grad():
        logits = model(imgs, dec_inp_ids, pad_token_id)
        loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))
    return loss.item()


# -------------------------------------------------------------
# main í•¨ìˆ˜ (ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸)
# -------------------------------------------------------------
def main():
    # GPU ìºì‹œ ì •ë¦¬
    torch.cuda.empty_cache()
    gc.collect()

    # ë¡œê·¸ ê¸°ë¡ í•¨ìˆ˜
    global tokenizer
    log_file = open("log.txt", "a", encoding="utf-8")
    def log(message):
        print(message)
        log_file.write(message + "\n")
        log_file.flush()

    # ------------------------- ë°ì´í„° ë¡œë“œ -------------------------
    data_dir = './data'
    tokenizer = JamoTokenizer()
    img_paths, labels = load_data(data_dir, tokenizer)

    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3),
    ])

    # ë°ì´í„°ì…‹ ë° ë¶„í• 
    dataset = OCRDataset(img_paths, labels, tokenizer, transform)
    total_size = len(dataset)
    indices = list(range(total_size))
    random.seed(42)
    random.shuffle(indices)
    split = int(total_size * 0.8)
    train_dataset = Subset(dataset, indices[:split])
    val_dataset = Subset(dataset, indices[split:])

    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)

    # ------------------------- ëª¨ë¸ ì„¤ì • -------------------------
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = JamoTrOCR(vocab_size=tokenizer.vocab_size).to(device)
    optimizer = optim.Adam(model.parameters(), lr=5e-5)
    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)

    # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
    save_dir = "./checkpoints"
    os.makedirs(save_dir, exist_ok=True)

    # ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
    latest_checkpoint_path = os.path.join(save_dir, "latest.pt")
    if os.path.exists(latest_checkpoint_path):
        model.load_state_dict(torch.load(latest_checkpoint_path, map_location=device))
        log("âœ… í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤. ì´ì–´ì„œ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.")
    else:
        log("ğŸš€ ìƒˆ ëª¨ë¸ë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    # ------------------------- í•™ìŠµ ë£¨í”„ -------------------------
    epochs = 200
    for epoch in range(epochs):
        log(f"\nEpoch {epoch+1}/{epochs}")

        model.train()
        train_loss = 0.0
        wrong_samples = []

        # -------- Training --------
        for step, (imgs, dec_inp_ids, labels) in enumerate(train_loader):
            loss, logits = train_step(model, optimizer, loss_fn, imgs, dec_inp_ids, labels, device, tokenizer.pad_token_id)
            train_loss += loss

            # ì˜¤ë‹µ ìƒ˜í”Œ ì¶”ì 
            with torch.no_grad():
                preds = logits.argmax(dim=-1).to(device)
                labels = labels.to(device)
                mask = (labels != -100).to(device)
                wrong_mask = (preds != labels) & mask
                batch_wrong = wrong_mask.any(dim=1).cpu()
                for i in range(len(batch_wrong)):
                    if batch_wrong[i]:
                        wrong_samples.append((imgs[i].cpu(), dec_inp_ids[i].cpu(), labels[i].cpu()))

            if step % 1000 == 0:
                log(f"[Train Step {step}/{len(train_loader)}] Loss: {loss:.4f}")
        train_loss /= len(train_loader)

        # -------- ì˜¤ë‹µ ì¬í•™ìŠµ --------
        if wrong_samples:
            log(f"\nì˜¤ë‹µ ìƒ˜í”Œ ìˆ˜: {len(wrong_samples)} â†’ ì¬í•™ìŠµ ì§„í–‰")
            wrong_loader = DataLoader(wrong_samples, batch_size=8, shuffle=True, collate_fn=collate_fn)
            for imgs, dec_inp_ids, labels in wrong_loader:
                train_step(model, optimizer, loss_fn, imgs, dec_inp_ids, labels, device, tokenizer.pad_token_id)

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            wrong_samples.clear()
            del wrong_loader
            torch.cuda.empty_cache()
            gc.collect()

        # -------- Validation --------
        val_loss = 0.0
        model.eval()
        with torch.no_grad():
            for imgs, dec_inp_ids, labels in val_loader:
                loss = eval_step(model, loss_fn, imgs, dec_inp_ids, labels, device, tokenizer.pad_token_id)
                val_loss += loss
        val_loss /= len(val_loader)

        log(f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

        # -------- Checkpoint ì €ì¥ --------
        epoch_checkpoint_path = os.path.join(save_dir, f"epoch_{epoch+1}.pt")
        torch.save(model.state_dict(), epoch_checkpoint_path)
        torch.save(model.state_dict(), latest_checkpoint_path)
        log(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {epoch_checkpoint_path}")

    log_file.close()


# -------------------------------------------------------------
# í”„ë¡œê·¸ë¨ ì‹¤í–‰
# -------------------------------------------------------------
if __name__ == "__main__":
    main()
